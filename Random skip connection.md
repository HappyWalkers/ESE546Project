# Random skip connection

## Abstract

Inspired by ResNet [1], Dropout [2], and DenseNet [3], we wonder if we could use random skip connections instead of stable skip connections in every residual block. Specifically, every short connection in DenseNet will be applied a probability that is subject to Bernoulli distribution with parameter $p$. Thus, we can adjust the $p$ to change the number of short connections in DenseNet to observe the effect of the number of short connections on the generalization of the model. 

To implement such a network, we come up with two alternative ways. In the first way, we can initialize a network with random skip connections in every basic block and then train the model until convergence. In the second way, we can randomly modify the structure, the skip connections, when training the network just like the dropout does. We need more experiments to decide which way to achieve higher accuracy with less training time.

## Related Work

Ashiotis et al. [4] have proposed the randomized skip connections to merge information of two domains. They applied $\psi$ that is subject to Bernoulli distribution with parameter $p$ to the first domain and applied $1 - \psi$ to the second domain when using shared weights at the same time. Gammell et al. [5] proposed a method to remove a connection in network with probability of $p$, the parameter of uniform distribution, and randomly draw a connection in network. They found the error rate decreased when the $p$ increased in experiment. However, they only did experiments on multilayer feedforward network. Shan et al. proposed a neural network with random connections via a scheme of a neural architecture search. First, a dense network was designed and trained to construct a search space, and then another network was generated by random sampling in the space. They claimed that their method achieved higher accuracy with less running time. However, their method can only be deployed on RNN-like model, while our idea will be applied to CNN-like model.

## Reference

[1] He, Kaiming, et al. "Deep residual learning." *Image Recognition* 7 (2015).

[2] Srivastava, Nitish, et al. "Dropout: a simple way to prevent neural networks from overfitting." *The journal of machine learning research* 15.1 (2014): 1929-1958.

[3] Huang, Gao, et al. "Densely connected convolutional networks." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2017.

[[4] Ashiotis, Giannis, et al. "Shared-Space Autoencoders with Randomized Skip Connections for Building Footprint Detection with Missing Views." *International Conference on Pattern Recognition*. Springer, Cham, 2021.](https://link.springer.com/chapter/10.1007/978-3-030-68787-8_39)

[[5] Gammell, Jimmy, et al. "Layer-Skipping Connections Improve the Effectiveness of Equilibrium Propagation on Layered Networks." *Frontiers in computational neuroscience* 15 (2021): 627357.](https://www.frontiersin.org/articles/10.3389/fncom.2021.627357/full)

[[6] Shan, Dongjing, et al. "Neural Architecture Search for a Highly Efficient Network with Random Skip Connections." *Applied Sciences* 10.11 (2020): 3712.](https://www.mdpi.com/2076-3417/10/11/3712)

